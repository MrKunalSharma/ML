"""
# K-Nearest Neighbors (KNN) Classifier

A custom implementation of the K-Nearest Neighbors algorithm supporting 
both Euclidean and Manhattan distance metrics.

Author: Kunal Sharma
Last Updated: 2023
"""

import numpy as np
import statistics
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier


class KNN_Classifier:
    """
    K-Nearest Neighbors classifier implementation.
    
    This implementation supports Euclidean and Manhattan distance metrics
    for classification tasks using the k-nearest neighbors algorithm.
    """

    def __init__(self, distance_metric='euclidean'):
        """
        Initialize the KNN classifier.
        
        Parameters:
        -----------
        distance_metric : str, default='euclidean'
            The distance metric to use ('euclidean' or 'manhattan')
        """
        self.distance_metric = distance_metric

    def get_distance_metric(self, training_data_point, test_data_point):
        """
        Calculate distance between training and test data points.
        
        Parameters:
        -----------
        training_data_point : array-like
            Single training data point with features and label
        test_data_point : array-like
            Single test data point with features
            
        Returns:
        --------
        float
            Distance between the points using specified metric
        """
        if self.distance_metric == 'euclidean':
            # Calculate Euclidean distance (L2 norm)
            dist = 0
            for i in range(len(training_data_point) - 1):
                dist += (training_data_point[i] - test_data_point[i])**2
            
            euclidean_dist = np.sqrt(dist)
            return euclidean_dist

        elif self.distance_metric == 'manhattan':
            # Calculate Manhattan distance (L1 norm)
            dist = 0
            for i in range(len(training_data_point) - 1):
                dist += abs(training_data_point[i] - test_data_point[i])
            
            manhattan_dist = dist
            return manhattan_dist
        
        else:
            raise ValueError("Distance metric must be 'euclidean' or 'manhattan'")

    def nearest_neighbors(self, X_train, test_data, k):
        """
        Find k nearest neighbors for a test data point.
        
        Parameters:
        -----------
        X_train : array-like
            Training data with features and labels
        test_data : array-like
            Single test data point
        k : int
            Number of neighbors to find
            
        Returns:
        --------
        list
            List of k nearest neighbors
        """
        distance_list = []

        # Calculate distances from test point to all training points
        for training_data in X_train:
            distance = self.get_distance_metric(training_data, test_data)
            distance_list.append((training_data, distance))

        # Sort by distance (ascending)
        distance_list.sort(key=lambda x: x[1])

        # Get the k nearest neighbors
        neighbors_list = []
        for j in range(min(k, len(distance_list))):
            neighbors_list.append(distance_list[j][0])

        return neighbors_list

    def predict(self, X_train, test_data, k):
        """
        Predict the class label for a test data point.
        
        Parameters:
        -----------
        X_train : array-like
            Training data with features and labels
        test_data : array-like
            Single test data point
        k : int
            Number of neighbors to consider
            
        Returns:
        --------
        predicted_class : same type as class labels
            Predicted class for the test data point
        """
        # Get k nearest neighbors
        neighbors = self.nearest_neighbors(X_train, test_data, k)

        # Extract the class labels of neighbors
        labels = [data[-1] for data in neighbors]

        # Return the most common class label
        try:
            predicted_class = statistics.mode(labels)
        except statistics.StatisticsError:
            # If there's a tie, return the first class in the tied group
            return labels[0]
            
        return predicted_class
    
    def evaluate(self, X_train, X_test, y_test, k):
        """
        Evaluate the KNN classifier on test data.
        
        Parameters:
        -----------
        X_train : array-like
            Training data with features and labels
        X_test : array-like
            Test data features
        y_test : array-like
            True labels for test data
        k : int
            Number of neighbors
            
        Returns:
        --------
        accuracy : float
            Classification accuracy (0 to 1)
        """
        predictions = []
        for test_point in X_test:
            predictions.append(self.predict(X_train, test_point, k))
        
        # Calculate accuracy
        accuracy = sum(predictions[i] == y_test[i] for i in range(len(y_test))) / len(y_test)
        return accuracy


def plot_decision_boundary(X, y, knn, k):
    """
    Plot the decision boundary for a 2D dataset.
    
    Parameters:
    -----------
    X : array-like, shape (n_samples, 2)
        2D feature data
    y : array-like, shape (n_samples,)
        Class labels
    knn : KNN_Classifier
        Trained KNN classifier
    k : int
        Number of neighbors to use
    """
    # Create a mesh grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))
    
    # Combine features
    Xgrid = np.c_[xx.ravel(), yy.ravel()]
    
    # Predict on mesh grid
    Z = np.array([knn.predict(np.c_[X, y], point, k) for point in Xgrid])
    Z = Z.reshape(xx.shape)
    
    # Plot decision boundary
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(f'KNN Decision Boundary (k={k}, metric={knn.distance_metric})')
    plt.show()


# Example Usage
if __name__ == "__main__":
    # Generate a simple dataset
    np.random.seed(0)
    X = np.random.randn(100, 2)  # 100 points, 2 features
    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Simple linear decision boundary
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Prepare data for our KNN implementation
    train_data = np.column_stack((X_train, y_train))
    
    # Initialize and test our KNN classifier
    for metric in ['euclidean', 'manhattan']:
        print(f"\nTesting with {metric} distance:")
        knn = KNN_Classifier(distance_metric=metric)
        
        # Test different k values
        for k in [1, 3, 5, 7]:
            # Our implementation
            custom_predictions = [knn.predict(train_data, test_point, k) for test_point in X_test]
            custom_accuracy = accuracy_score(y_test, custom_predictions)
            
            # Scikit-learn implementation for comparison
            sklearn_knn = KNeighborsClassifier(n_neighbors=k, metric=metric)
            sklearn_knn.fit(X_train, y_train)
            sklearn_predictions = sklearn_knn.predict(X_test)
            sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)
            
            print(f"k={k}:")
            print(f"  Custom implementation accuracy: {custom_accuracy:.4f}")
            print(f"  Scikit-learn accuracy: {sklearn_accuracy:.4f}")
    
    # Visualization example (only works with 2D data)
    knn = KNN_Classifier(distance_metric='euclidean')
    plot_decision_boundary(X, y, knn, k=5)
